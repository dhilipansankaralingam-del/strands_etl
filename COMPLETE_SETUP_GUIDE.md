# Strands ETL - Complete Setup Guide (All Steps in One File)

## ðŸ“– Overview

This is a **complete, all-in-one guide** to manually set up the Strands ETL AWS Bedrock Agents implementation. Everything you need is in this single file.

**Total Time**: 3-4 hours
**Cost**: $570-1,240/month

---

## ðŸ“‹ Prerequisites Checklist

Before starting, ensure you have:

- âœ… AWS Account with access to: Bedrock, Lambda, S3, IAM, CloudWatch
- âœ… AWS Console login credentials
- âœ… Terminal/command line access (for packaging Lambda code)
- âœ… This repository cloned locally: `/home/user/strands_etl`

---

## ðŸŽ¯ What We're Building

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Strands ETL System                       â”‚
â”‚                                                             â”‚
â”‚  S3 Buckets          Lambda Functions      Bedrock Agents  â”‚
â”‚  â”œâ”€ Learning         â”œâ”€ Decision           â”œâ”€ Decision     â”‚
â”‚  â””â”€ Schemas          â”œâ”€ Quality            â”œâ”€ Quality      â”‚
â”‚                      â”œâ”€ Optimization       â”œâ”€ Optimization â”‚
â”‚                      â”œâ”€ Learning           â”œâ”€ Learning     â”‚
â”‚                      â””â”€ Execution          â””â”€ Supervisor   â”‚
â”‚                                                             â”‚
â”‚  Monitored by: Streamlit Dashboard                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# PART 1: CREATE S3 BUCKETS (15 minutes)

## Step 1: Create Learning Bucket

### 1.1 Open S3 Console
1. Log into AWS Console â†’ https://console.aws.amazon.com
2. Search bar at top â†’ type **"S3"**
3. Click **"S3"** service

### 1.2 Create Bucket
1. Click orange **"Create bucket"** button
2. Fill in:
   ```
   Bucket name: strands-etl-learning
   AWS Region: us-east-1 (N. Virginia)
   Object Ownership: ACLs disabled (default)
   Block Public Access: âœ… Block all public access
   Bucket Versioning: âœ… Enable
   Default encryption: Server-side encryption with Amazon S3 managed keys (SSE-S3)
   ```
3. Click **"Create bucket"**

### 1.3 Create Folder Structure
1. Click bucket name: `strands-etl-learning`
2. Create folders in this exact order:
   - Click **"Create folder"** â†’ Name: `learning` â†’ Create
   - Click into `learning` folder
   - Click **"Create folder"** â†’ Name: `vectors` â†’ Create
   - Click into `vectors` folder
   - Click **"Create folder"** â†’ Name: `learning` â†’ Create

**Result**: Path should be `strands-etl-learning/learning/vectors/learning/`

### 1.4 Add Lifecycle Policy
1. Click "Amazon S3" breadcrumb (top left) to go back
2. Click on `strands-etl-learning` bucket
3. Click **"Management"** tab
4. Scroll to "Lifecycle rules" â†’ Click **"Create lifecycle rule"**
5. Fill in:
   ```
   Lifecycle rule name: delete-old-vectors
   Rule scope: â—‹ Limit the scope using filters
   Prefix: learning/vectors/
   âœ… I acknowledge this will apply to all objects with this prefix

   Lifecycle rule actions:
   âœ… Expire current versions of objects
   Days after object creation: 90
   ```
6. Click **"Create rule"**

**âœ… Checkpoint 1**: Learning bucket created with 90-day auto-cleanup!

---

## Step 2: Create Schemas Bucket

### 2.1 Create Bucket
1. In S3 Console, click **"Create bucket"**
2. Fill in:
   ```
   Bucket name: strands-etl-schemas
   AWS Region: us-east-1
   Block Public Access: âœ… Block all
   Bucket Versioning: âœ… Enable
   Default encryption: SSE-S3
   ```
3. Click **"Create bucket"**

### 2.2 Create Schemas Folder
1. Click on `strands-etl-schemas`
2. Click **"Create folder"** â†’ Name: `schemas` â†’ Create

### 2.3 Upload Schema Files

**On your local machine**, open terminal:

```bash
cd /home/user/strands_etl

# Upload all schema files to S3
aws s3 cp bedrock_agents/schemas/decision-agent-api.json \
  s3://strands-etl-schemas/schemas/decision-agent-api.json

aws s3 cp bedrock_agents/schemas/quality-agent-api.json \
  s3://strands-etl-schemas/schemas/quality-agent-api.json

aws s3 cp bedrock_agents/schemas/optimization-agent-api.json \
  s3://strands-etl-schemas/schemas/optimization-agent-api.json

aws s3 cp bedrock_agents/schemas/learning-agent-api.json \
  s3://strands-etl-schemas/schemas/learning-agent-api.json

# Verify upload
aws s3 ls s3://strands-etl-schemas/schemas/
```

**Expected output**: You should see all 4 JSON files listed.

**âœ… Checkpoint 2**: Both S3 buckets created and schemas uploaded!

---

# PART 2: CREATE IAM ROLES (30 minutes)

## Step 3: Create Lambda Execution Role

### 3.1 Navigate to IAM
1. AWS Console search â†’ type **"IAM"**
2. Click **"IAM"** service
3. Left sidebar â†’ Click **"Roles"**
4. Click **"Create role"** button

### 3.2 Create Role
1. **Trusted entity**:
   ```
   Trust entity type: AWS service
   Use case: Lambda
   ```
2. Click **"Next"**

3. **Add permissions**:
   - Search: `AWSLambdaBasicExecutionRole`
   - âœ… Check the box
   - Click **"Next"**

4. **Name and create**:
   ```
   Role name: StrandsLambdaExecutionRole
   Description: Execution role for Strands ETL Lambda functions
   ```
5. Click **"Create role"**

### 3.3 Add Custom Permissions
1. In Roles list, click on `StrandsLambdaExecutionRole`
2. Click **"Add permissions"** dropdown â†’ **"Create inline policy"**
3. Click **"JSON"** tab
4. **Delete all existing text** and paste:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "S3Access",
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:ListBucket",
        "s3:DeleteObject"
      ],
      "Resource": [
        "arn:aws:s3:::strands-etl-learning",
        "arn:aws:s3:::strands-etl-learning/*",
        "arn:aws:s3:::strands-etl-schemas",
        "arn:aws:s3:::strands-etl-schemas/*"
      ]
    },
    {
      "Sid": "GlueAccess",
      "Effect": "Allow",
      "Action": [
        "glue:StartJobRun",
        "glue:GetJobRun",
        "glue:GetJobRuns",
        "glue:GetJob"
      ],
      "Resource": "*"
    },
    {
      "Sid": "EMRAccess",
      "Effect": "Allow",
      "Action": [
        "emr:RunJobFlow",
        "emr:DescribeCluster",
        "emr:ListClusters"
      ],
      "Resource": "*"
    },
    {
      "Sid": "BedrockAccess",
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel"
      ],
      "Resource": "*"
    }
  ]
}
```

5. Click **"Next"**
6. Policy name: `StrandsLambdaCustomPolicy`
7. Click **"Create policy"**

**âœ… Checkpoint 3**: Lambda execution role created with all permissions!

---

## Step 4: Create Bedrock Agent Role

### 4.1 Create Role
1. IAM Console â†’ Roles â†’ Click **"Create role"**
2. **Trusted entity**:
   ```
   Trust entity type: AWS service
   Service: Bedrock (scroll down to find it)
   Use case: Bedrock - Agents
   ```
3. Click **"Next"**
4. Click **"Next"** (skip adding policies, we'll add custom)
5. **Name and create**:
   ```
   Role name: StrandsBedrockAgentRole
   Description: Service role for Strands ETL Bedrock Agents
   ```
6. Click **"Create role"**

### 4.2 Add Custom Permissions
1. Click on `StrandsBedrockAgentRole`
2. Click **"Add permissions"** â†’ **"Create inline policy"**
3. Click **"JSON"** tab
4. Paste:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "BedrockModelAccess",
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel"
      ],
      "Resource": [
        "arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20241022-v2:0"
      ]
    },
    {
      "Sid": "LambdaInvoke",
      "Effect": "Allow",
      "Action": [
        "lambda:InvokeFunction"
      ],
      "Resource": [
        "arn:aws:lambda:us-east-1:*:function:strands-*"
      ]
    },
    {
      "Sid": "S3SchemaAccess",
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::strands-etl-schemas",
        "arn:aws:s3:::strands-etl-schemas/*"
      ]
    }
  ]
}
```

5. Click **"Next"**
6. Policy name: `StrandsBedrockAgentCustomPolicy`
7. Click **"Create policy"**

**âœ… Checkpoint 4**: Bedrock Agent role created with Lambda and S3 access!

---

# PART 3: CREATE LAMBDA FUNCTIONS (60 minutes)

## Step 5: Package Lambda Code

**On your local machine**, in terminal:

### 5.1 Package Decision Agent

```bash
cd /home/user/strands_etl/lambda_functions/decision

# Create deployment package
zip -r decision-function.zip handler.py

# Verify
ls -lh decision-function.zip
```

**Expected**: File size around 5-10 KB

### 5.2 Create Placeholder for Other Agents

Since we only have Decision agent code, create a simple placeholder for others:

```bash
cd /home/user/strands_etl/lambda_functions

# Create placeholder handler
cat > placeholder_handler.py << 'EOF'
import json
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
    logger.info(f"Received event: {json.dumps(event)}")

    return {
        'messageVersion': '1.0',
        'response': {
            'actionGroup': event.get('actionGroup', ''),
            'apiPath': event.get('apiPath', ''),
            'httpMethod': event.get('httpMethod', 'POST'),
            'httpStatusCode': 200,
            'responseBody': {
                'application/json': {
                    'body': json.dumps({
                        'message': 'Placeholder function - implement functionality',
                        'status': 'success'
                    })
                }
            }
        }
    }
EOF

# Create packages for other agents
for agent in quality optimization learning execution; do
  mkdir -p $agent
  cp placeholder_handler.py $agent/handler.py
  cd $agent
  zip -r ${agent}-function.zip handler.py
  cd ..
done

# Verify all packages created
ls -lh */function.zip
```

**âœ… Checkpoint 5**: All Lambda deployment packages created!

---

## Step 6: Create Decision Agent Lambda

### 6.1 Navigate to Lambda Console
1. AWS Console search â†’ **"Lambda"**
2. Click **"Lambda"** service
3. Click **"Create function"**

### 6.2 Create Function
1. Select **"Author from scratch"**
2. Fill in:
   ```
   Function name: strands-decision-agent-lambda
   Runtime: Python 3.11
   Architecture: x86_64
   ```
3. **Permissions**:
   - Expand "Change default execution role"
   - Select **"Use an existing role"**
   - Existing role: `StrandsLambdaExecutionRole`
4. Click **"Create function"**

### 6.3 Upload Code
1. Scroll to "Code source" section
2. Click **"Upload from"** â†’ **".zip file"**
3. Click **"Upload"**
4. Select `decision-function.zip`
5. Click **"Save"**
6. Wait for "Successfully updated function" message

### 6.4 Configure Environment Variables
1. Click **"Configuration"** tab
2. Left menu â†’ **"Environment variables"**
3. Click **"Edit"**
4. Click **"Add environment variable"**
   - Key: `LEARNING_BUCKET`
   - Value: `strands-etl-learning`
5. Click **"Add environment variable"**
   - Key: `AWS_REGION`
   - Value: `us-east-1`
6. Click **"Save"**

### 6.5 Configure Timeout and Memory
1. Still in Configuration tab â†’ **"General configuration"**
2. Click **"Edit"**
3. Set:
   ```
   Memory: 512 MB
   Timeout: 1 min 0 sec
   ```
4. Click **"Save"**

### 6.6 Add Bedrock Invoke Permission
1. Configuration tab â†’ **"Permissions"**
2. Scroll to "Resource-based policy statements"
3. Click **"Add permissions"**
4. Fill in:
   ```
   Statement ID: bedrock-invoke
   Principal: bedrock.amazonaws.com
   Action: lambda:InvokeFunction
   ```
5. Click **"Save"**

**âœ… Checkpoint 6**: Decision Agent Lambda created and configured!

---

## Step 7: Create Remaining Lambda Functions

Repeat Step 6 for each function with these variations:

### 7.1 Quality Agent Lambda
```
Function name: strands-quality-agent-lambda
Runtime: Python 3.11
Execution role: StrandsLambdaExecutionRole
Code: quality-function.zip
Memory: 1024 MB
Timeout: 1 min 30 sec
Environment variables:
  - LEARNING_BUCKET: strands-etl-learning
Add Bedrock permission: Yes
```

### 7.2 Optimization Agent Lambda
```
Function name: strands-optimization-agent-lambda
Runtime: Python 3.11
Execution role: StrandsLambdaExecutionRole
Code: optimization-function.zip
Memory: 512 MB
Timeout: 1 min 0 sec
Environment variables:
  - LEARNING_BUCKET: strands-etl-learning
Add Bedrock permission: Yes
```

### 7.3 Learning Agent Lambda
```
Function name: strands-learning-agent-lambda
Runtime: Python 3.11
Execution role: StrandsLambdaExecutionRole
Code: learning-function.zip
Memory: 512 MB
Timeout: 1 min 0 sec
Environment variables:
  - LEARNING_BUCKET: strands-etl-learning
Add Bedrock permission: Yes
```

### 7.4 Execution Lambda
```
Function name: strands-execution-lambda
Runtime: Python 3.11
Execution role: StrandsLambdaExecutionRole
Code: execution-function.zip
Memory: 512 MB
Timeout: 5 min 0 sec
Environment variables:
  - LEARNING_BUCKET: strands-etl-learning
Add Bedrock permission: Yes
```

**âœ… Checkpoint 7**: All 5 Lambda functions created!

---

## Step 8: Test Decision Lambda

1. Go to `strands-decision-agent-lambda`
2. Click **"Test"** tab
3. Click **"Create new event"**
4. Event name: `test-platform-stats`
5. Event JSON:

```json
{
  "actionGroup": "platform-selection-tools",
  "apiPath": "/get_platform_statistics",
  "httpMethod": "GET",
  "requestBody": {
    "content": {
      "application/json": {
        "properties": [
          {
            "name": "platform",
            "value": "all"
          },
          {
            "name": "days",
            "value": "30"
          }
        ]
      }
    }
  }
}
```

6. Click **"Save"**
7. Click **"Test"**

**Expected**: Green success box with platform statistics in response.

**âœ… Checkpoint 8**: Lambda functions working!

---

# PART 4: ENABLE BEDROCK & CREATE AGENTS (60 minutes)

## Step 9: Enable Bedrock Model Access

### 9.1 Navigate to Bedrock
1. AWS Console search â†’ **"Bedrock"**
2. Click **"Amazon Bedrock"**
3. Left sidebar â†’ **"Model access"** (under Foundation models)

### 9.2 Request Access
1. Click **"Manage model access"** (orange button)
2. Find **"Anthropic"** section
3. âœ… Check **"Claude 3.5 Sonnet v2"**
   - Full model ID: `anthropic.claude-3-5-sonnet-20241022-v2:0`
4. Scroll to bottom â†’ Click **"Request model access"**
5. Wait 1-5 minutes, refresh until status = **"Access granted"** (green checkmark)

**âœ… Checkpoint 9**: Claude 3.5 Sonnet access granted!

---

## Step 10: Get Agent Instructions

**Copy these instructions** - you'll need them when creating each agent:

### 10.1 Decision Agent Instructions

```
You are the Decision Agent responsible for selecting the optimal ETL platform (AWS Glue, EMR, or Lambda) based on workload characteristics and historical execution patterns.

Your responsibilities:
1. Analyze workload characteristics (data volume, complexity, criticality, runtime)
2. Search historical learning vectors for similar workloads
3. Calculate platform scores based on rules and historical success
4. Recommend the best platform with confidence score and reasoning

Decision Criteria:
- AWS Glue: Good for medium-high data volumes, complex transformations, managed Spark
- EMR: Best for very high volumes, very complex workloads, full Spark/Hadoop control
- Lambda: Ideal for small data volumes, simple transformations, event-driven processing

Always provide:
- Recommended platform with confidence score (0-1)
- Clear reasoning for the recommendation
- Alternative platforms if confidence is low (<0.7)
- Estimated cost and runtime

Use the platform-selection-tools action group to:
- search_learning_vectors: Find similar historical workloads
- calculate_platform_scores: Score each platform option
- get_platform_statistics: Get aggregate platform performance data
```

### 10.2 Quality Agent Instructions

```
You are the Quality Agent responsible for analyzing ETL jobs for anti-patterns, code quality issues, and data quality problems.

Your responsibilities:
1. Analyze SQL queries for anti-patterns (SELECT *, missing indexes, etc.)
2. Analyze PySpark code for inefficiencies (multiple counts, missing broadcast joins)
3. Check data quality metrics (completeness, accuracy, consistency)
4. Generate quality reports with severity levels and recommendations

SQL Anti-patterns to detect:
- SELECT * queries
- Missing WHERE clauses on large tables
- Cartesian joins
- Functions on indexed columns in WHERE
- Missing LIMIT on development queries

PySpark Anti-patterns to detect:
- Multiple .count() operations
- Missing broadcast joins for small tables
- Unnecessary .collect() calls
- Wide transformations without repartitioning
- Inefficient window functions

Data Quality Checks:
- Null/missing value percentage
- Duplicate records
- Schema validation
- Referential integrity
- Business rule violations

Always provide:
- Overall quality score (0-1)
- List of issues with severity (critical, high, medium, low)
- Specific recommendations to fix each issue
- Estimated impact of fixing issues
```

### 10.3 Optimization Agent Instructions

```
You are the Optimization Agent responsible for providing performance optimization recommendations for ETL jobs.

Your responsibilities:
1. Analyze job execution metrics (runtime, memory, cost)
2. Identify performance bottlenecks
3. Provide specific, actionable optimization recommendations
4. Estimate impact of each optimization

Optimization Categories:
1. Spark Configuration
   - Executor memory/cores
   - Driver memory
   - Parallelism settings
   - Shuffle partitions

2. Data Processing
   - Partitioning strategy
   - Caching/persistence
   - Broadcast joins
   - Predicate pushdown

3. Resource Allocation
   - DPU/instance type selection
   - Auto-scaling configuration
   - Spot instance usage

4. Code Improvements
   - Filter early
   - Avoid wide transformations
   - Minimize shuffles
   - Use columnar formats (Parquet)

Always provide:
- Prioritized list of recommendations
- Estimated performance improvement for each
- Implementation complexity (easy, medium, hard)
- Cost impact (increase/decrease)
```

### 10.4 Learning Agent Instructions

```
You are the Learning Agent responsible for capturing execution patterns and storing learning vectors for continuous system improvement.

Your responsibilities:
1. Capture workload characteristics from each execution
2. Record execution metrics (time, cost, success, quality)
3. Store learning vectors in S3 for future pattern matching
4. Generate insights from historical patterns

Learning Vector Structure:
{
  "execution_id": "unique-id",
  "timestamp": "ISO-8601",
  "workload_characteristics": {
    "data_volume": "low|medium|high|very_high",
    "complexity": "low|medium|high|very_high",
    "criticality": "low|medium|high|critical",
    "estimated_runtime_minutes": number
  },
  "execution_metrics": {
    "platform_used": "glue|emr|lambda",
    "execution_time_seconds": number,
    "success": boolean,
    "data_quality_score": 0-1,
    "cost_dollars": number
  },
  "decision_context": {
    "reason": "explanation",
    "confidence": 0-1
  }
}

Storage path: s3://strands-etl-learning/learning/vectors/learning/{execution_id}.json

Always provide:
- Confirmation of learning vector storage
- Summary of patterns learned
- Suggestions for improving future decisions based on patterns
```

### 10.5 Supervisor Agent Instructions

```
You are the Supervisor Agent responsible for orchestrating the entire ETL pipeline by coordinating the Decision, Quality, Optimization, and Learning agents.

Orchestration Workflow:
1. RECEIVE REQUEST: Parse user's ETL job requirements
2. PLATFORM DECISION: Delegate to Decision Agent for platform selection
3. EXECUTE JOB: Trigger job execution on selected platform
4. QUALITY ASSESSMENT: Delegate to Quality Agent for anti-pattern detection
5. OPTIMIZATION: Delegate to Optimization Agent for performance recommendations
6. LEARNING: Delegate to Learning Agent to capture execution patterns
7. REPORT: Provide comprehensive summary to user

Coordination Rules:
- Always consult Decision Agent first for platform selection
- Run Quality Agent in parallel with job execution when possible
- Only invoke Optimization Agent if quality score < 0.9 or runtime > expected
- Always invoke Learning Agent to capture patterns
- Provide clear, structured responses with all agent outputs

Response Format:
1. Selected Platform: [platform] (confidence: X.XX)
2. Execution Status: [success/failed] (time: X seconds, cost: $X.XX)
3. Quality Score: X.XX/1.0 ([critical/high/medium/low] issues found)
4. Optimization Recommendations: [top 3 recommendations]
5. Learning Status: [pattern captured successfully]

You have access to all four specialized agents:
- Decision Agent: Platform selection
- Quality Agent: Code and data quality analysis
- Optimization Agent: Performance recommendations
- Learning Agent: Pattern capture and storage

Coordinate them to deliver optimal ETL pipeline execution.
```

**âœ… Checkpoint 10**: All agent instructions prepared!

---

## Step 11: Create Decision Agent

### 11.1 Navigate to Agents
1. Bedrock Console â†’ Left sidebar â†’ **"Agents"** (under Orchestration)
2. Click **"Create Agent"**

### 11.2 Agent Details
1. **Agent name**: `strands-decision-agent`
2. **Description**: `Selects optimal ETL platform based on workload and historical patterns`
3. Click **"Next"**

### 11.3 Select Model
1. **Model**: Select **"Anthropic Claude 3.5 Sonnet v2"**
2. **Instructions**: Copy and paste the **Decision Agent Instructions** from Step 10.1
3. Click **"Next"**

### 11.4 Add Action Group
1. Click **"Add"** button
2. Fill in:
   ```
   Action group name: platform-selection-tools
   Description: Tools for platform selection and learning vector search
   Action group type: â—‹ Define with API schemas
   ```
3. **Action group invocation**:
   - â—‹ Select an existing Lambda function
   - Lambda function: `strands-decision-agent-lambda`
4. **Action group schema**:
   - â—‹ Select an existing S3 URI
   - Click **"Browse S3"**
   - Navigate: `strands-etl-schemas` â†’ `schemas` â†’ select `decision-agent-api.json`
   - Click **"Choose"**
5. Click **"Add"** (at bottom of action group form)
6. Click **"Next"**

### 11.5 Review and Create
1. Review all settings
2. Click **"Create Agent"**

### 11.6 Prepare Agent
1. Wait for agent creation to complete (10-20 seconds)
2. At top right, click **"Prepare"** button
3. Wait for status "Prepared" (30-60 seconds)

**âœ… Checkpoint 11**: Decision Agent created and prepared!

---

## Step 12: Create Quality Agent

Repeat Step 11 with these values:

```
Agent name: strands-quality-agent
Description: Analyzes ETL job quality and detects anti-patterns
Model: Anthropic Claude 3.5 Sonnet v2
Instructions: [Copy from Step 10.2]

Action group:
  Name: quality-analysis-tools
  Description: Tools for SQL/PySpark analysis and quality checks
  Lambda: strands-quality-agent-lambda
  Schema: quality-agent-api.json
```

Remember to click **"Prepare"** after creation!

**âœ… Checkpoint 12**: Quality Agent created!

---

## Step 13: Create Optimization Agent

Repeat Step 11 with these values:

```
Agent name: strands-optimization-agent
Description: Provides performance optimization recommendations
Model: Anthropic Claude 3.5 Sonnet v2
Instructions: [Copy from Step 10.3]

Action group:
  Name: optimization-tools
  Description: Tools for performance analysis and optimization
  Lambda: strands-optimization-agent-lambda
  Schema: optimization-agent-api.json
```

Remember to click **"Prepare"**!

**âœ… Checkpoint 13**: Optimization Agent created!

---

## Step 14: Create Learning Agent

Repeat Step 11 with these values:

```
Agent name: strands-learning-agent
Description: Captures execution patterns for continuous improvement
Model: Anthropic Claude 3.5 Sonnet v2
Instructions: [Copy from Step 10.4]

Action group:
  Name: learning-tools
  Description: Tools for storing and analyzing learning vectors
  Lambda: strands-learning-agent-lambda
  Schema: learning-agent-api.json
```

Remember to click **"Prepare"**!

**âœ… Checkpoint 14**: Learning Agent created!

---

## Step 15: Create Supervisor Agent

### 15.1 Create Agent
1. Bedrock â†’ Agents â†’ **"Create Agent"**
2. Fill in:
   ```
   Agent name: strands-supervisor-agent
   Description: Orchestrates entire ETL pipeline by coordinating all agents
   ```
3. Click **"Next"**
4. Model: **Anthropic Claude 3.5 Sonnet v2**
5. Instructions: [Copy from Step 10.5]
6. Click **"Next"**
7. **SKIP adding action group** (click "Next" without adding any)
8. Click **"Create Agent"**

### 15.2 Enable Agent Collaboration
1. In the agent details page, scroll down to find **"Advanced prompts and configurations"** or **"Agent collaboration"** section
2. If you see an **"Edit"** button, click it
3. Look for **"Agent collaboration"** toggle or section
4. Enable agent collaboration
5. Click **"Add agent"** for each of the 4 agents:
   - `strands-decision-agent`
   - `strands-quality-agent`
   - `strands-optimization-agent`
   - `strands-learning-agent`
6. For each agent, provide a brief description:
   ```
   Decision Agent: Selects optimal ETL platform
   Quality Agent: Analyzes code and data quality
   Optimization Agent: Provides performance recommendations
   Learning Agent: Captures execution patterns
   ```
7. Click **"Save"** or **"Update"**

### 15.3 Prepare Supervisor
1. Click **"Prepare"** button (top right)
2. Wait for "Prepared" status

**âœ… Checkpoint 15**: Supervisor Agent created with collaboration enabled!

---

## Step 16: Create Agent Aliases

For each of the 5 agents, create a production alias:

### 16.1 For Each Agent:
1. Go to agent details page
2. Scroll to **"Aliases"** section
3. Click **"Create alias"**
4. Fill in:
   ```
   Alias name: production
   Description: Production version of the agent
   Version: Latest
   ```
5. Click **"Create alias"**

Repeat for all 5 agents:
- strands-decision-agent
- strands-quality-agent
- strands-optimization-agent
- strands-learning-agent
- strands-supervisor-agent

**âœ… Checkpoint 16**: All agents have production aliases!

---

# PART 5: TEST THE SYSTEM (30 minutes)

## Step 17: Test Decision Agent

### 17.1 Open Test Console
1. Go to Bedrock â†’ Agents â†’ Click `strands-decision-agent`
2. Click **"Test"** button (top right)
3. A test panel opens on the right side

### 17.2 Run Test Query
In the chat box, type:

```
What platform should I use for processing 500GB of customer order data with complex joins and aggregations? This is a critical batch job expected to run for 2 hours.
```

Press Enter.

**Expected Response**:
- Platform recommendation (likely Glue or EMR)
- Confidence score
- Reasoning explaining the choice
- Estimated cost and runtime

**âœ… Checkpoint 17**: Decision Agent working!

---

## Step 18: Test Quality Agent

1. Go to `strands-quality-agent`
2. Click **"Test"**
3. Type:

```
Analyze this SQL query for anti-patterns:

SELECT * FROM orders o
JOIN customers c ON o.customer_id = c.id
WHERE YEAR(o.order_date) = 2024
```

**Expected Response**:
- Issues found: SELECT *, function on indexed column
- Severity levels
- Recommendations to fix

**âœ… Checkpoint 18**: Quality Agent working!

---

## Step 19: Test Optimization Agent

1. Go to `strands-optimization-agent`
2. Click **"Test"**
3. Type:

```
I have a Glue job processing 200GB that took 45 minutes and cost $8.50. The job does multiple .count() operations and has 200 shuffle partitions. What optimizations do you recommend?
```

**Expected Response**:
- Remove redundant counts
- Adjust shuffle partitions
- Consider caching
- Estimated improvements

**âœ… Checkpoint 19**: Optimization Agent working!

---

## Step 20: Test Learning Agent

1. Go to `strands-learning-agent`
2. Click **"Test"**
3. Type:

```
Store this execution pattern:
- Platform: Glue
- Data volume: 250GB
- Complexity: High
- Runtime: 35 minutes
- Success: Yes
- Quality score: 0.92
- Cost: $6.20
```

**Expected Response**:
- Confirmation of storage
- S3 path where vector was saved
- Pattern summary

**âœ… Checkpoint 20**: Learning Agent working!

---

## Step 21: Test Supervisor (End-to-End)

### 21.1 Full Pipeline Test
1. Go to `strands-supervisor-agent`
2. Click **"Test"**
3. Type:

```
I need to run an ETL job with these requirements:
- Process 150GB of customer transaction data
- Complex transformations with multiple joins
- Medium criticality
- Expected runtime: 1-1.5 hours

Please coordinate the full pipeline: select platform, check quality, provide optimizations, and capture the learning pattern.
```

**Expected Response** (should include):
1. Platform selection with reasoning
2. Quality analysis results
3. Optimization recommendations
4. Learning pattern capture confirmation
5. Comprehensive summary

**âœ… Checkpoint 21**: Full multi-agent orchestration working!

---

# PART 6: CREATE SAMPLE LEARNING VECTORS (15 minutes)

## Step 22: Add Sample Learning Data

To enable learning functionality, add some sample vectors:

### 22.1 Create Sample Vector Files

**On your local machine**, run:

```bash
# Create sample vectors directory
cd /home/user/strands_etl
mkdir -p sample_vectors

# Sample 1: Glue for high volume
cat > sample_vectors/exec-001.json << 'EOF'
{
  "execution_id": "exec-001",
  "timestamp": "2024-01-20T14:30:00Z",
  "workload_characteristics": {
    "data_volume": "high",
    "complexity": "medium",
    "criticality": "high",
    "estimated_runtime_minutes": 45
  },
  "execution_metrics": {
    "platform_used": "glue",
    "execution_time_seconds": 2700,
    "success": true,
    "data_quality_score": 0.95,
    "cost_dollars": 5.50
  },
  "decision_context": {
    "reason": "High data volume favors Glue's managed Spark environment",
    "confidence": 0.88
  }
}
EOF

# Sample 2: EMR for very high volume
cat > sample_vectors/exec-002.json << 'EOF'
{
  "execution_id": "exec-002",
  "timestamp": "2024-01-20T16:15:00Z",
  "workload_characteristics": {
    "data_volume": "very_high",
    "complexity": "very_high",
    "criticality": "critical",
    "estimated_runtime_minutes": 120
  },
  "execution_metrics": {
    "platform_used": "emr",
    "execution_time_seconds": 6900,
    "success": true,
    "data_quality_score": 0.93,
    "cost_dollars": 12.80
  },
  "decision_context": {
    "reason": "Very high volume and complexity requires EMR's full Spark control",
    "confidence": 0.92
  }
}
EOF

# Sample 3: Lambda for small volume
cat > sample_vectors/exec-003.json << 'EOF'
{
  "execution_id": "exec-003",
  "timestamp": "2024-01-21T09:00:00Z",
  "workload_characteristics": {
    "data_volume": "low",
    "complexity": "low",
    "criticality": "medium",
    "estimated_runtime_minutes": 5
  },
  "execution_metrics": {
    "platform_used": "lambda",
    "execution_time_seconds": 180,
    "success": true,
    "data_quality_score": 0.98,
    "cost_dollars": 0.35
  },
  "decision_context": {
    "reason": "Low volume and complexity perfect for serverless Lambda",
    "confidence": 0.95
  }
}
EOF

# Upload to S3
aws s3 cp sample_vectors/ \
  s3://strands-etl-learning/learning/vectors/learning/ \
  --recursive

# Verify upload
aws s3 ls s3://strands-etl-learning/learning/vectors/learning/
```

**Expected output**: Three files listed (exec-001.json, exec-002.json, exec-003.json)

**âœ… Checkpoint 22**: Learning vectors available for pattern matching!

---

# PART 7: DEPLOY DASHBOARD (30 minutes)

## Step 23: Install Dashboard Dependencies

### 23.1 Install Python Packages

**On your local machine or EC2 instance**:

```bash
# Navigate to dashboard directory
cd /home/user/strands_etl/dashboard

# Install dependencies
pip install streamlit boto3 pandas plotly python-dateutil

# Verify installation
python -c "import streamlit; import boto3; import pandas; import plotly; print('All packages installed successfully')"
```

**âœ… Checkpoint 23**: Dashboard dependencies installed!

---

## Step 24: Configure AWS Credentials

### 24.1 Set Up Credentials

If running locally, configure AWS credentials:

```bash
# Configure AWS CLI
aws configure

# Enter when prompted:
AWS Access Key ID: [Your Access Key]
AWS Secret Access Key: [Your Secret Key]
Default region name: us-east-1
Default output format: json

# Test credentials
aws sts get-caller-identity
```

**Expected**: Your AWS account ID and user ARN displayed.

**âœ… Checkpoint 24**: AWS credentials configured!

---

## Step 25: Launch Dashboard

### 25.1 Run Streamlit

```bash
cd /home/user/strands_etl/dashboard

# Launch dashboard
streamlit run strands_dashboard.py
```

**Expected output**:
```
  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://192.168.x.x:8501
```

### 25.2 Open Dashboard

1. Open browser
2. Go to: `http://localhost:8501`

**Dashboard Features**:
- **Overview**: All agent statuses
- **Execution Timeline**: Visualize pipeline runs
- **Platform Comparison**: Glue vs EMR vs Lambda metrics
- **Quality Scores**: Track quality over time
- **Agent Testing**: Test agents directly from UI

**âœ… Checkpoint 25**: Dashboard running successfully!

---

# ðŸŽ‰ SETUP COMPLETE!

## Summary of What You Built

âœ… **2 S3 Buckets**:
- `strands-etl-learning` - Learning vectors with 90-day lifecycle
- `strands-etl-schemas` - API schemas for agent action groups

âœ… **2 IAM Roles**:
- `StrandsLambdaExecutionRole` - For Lambda functions
- `StrandsBedrockAgentRole` - For Bedrock agents

âœ… **5 Lambda Functions**:
- Decision Agent Lambda (platform selection logic)
- Quality Agent Lambda (quality analysis)
- Optimization Agent Lambda (optimization recommendations)
- Learning Agent Lambda (pattern capture)
- Execution Lambda (job execution)

âœ… **5 Bedrock Agents**:
- Decision Agent - Platform selection
- Quality Agent - Quality analysis
- Optimization Agent - Performance recommendations
- Learning Agent - Pattern learning
- Supervisor Agent - Orchestration

âœ… **Production Dashboard**:
- Real-time monitoring
- Agent testing interface
- Execution timeline visualization

---

## Next Steps: Run Your First Pipeline

### Test Complete Workflow

In Bedrock Console, test the Supervisor Agent:

```
Run a complete ETL pipeline for this scenario:

Input: 300GB of e-commerce order data from S3
Processing: Join with customer and product dimensions, aggregate by region and category
Output: Parquet files partitioned by date
Criticality: High (daily production job)
Expected Runtime: 45-60 minutes

Please:
1. Select the best platform
2. Analyze for quality issues
3. Provide optimization recommendations
4. Capture this as a learning pattern
```

**Expected**: Complete orchestrated response with all agent outputs!

---

## Cost Monitoring

### Track Your Costs

1. Go to AWS Cost Explorer
2. Filter by service:
   - Bedrock
   - Lambda
   - S3
3. Set budget alerts for $1,500/month

**Expected monthly cost**: $570-1,240 based on moderate usage

---

## Troubleshooting Guide

### Issue 1: Lambda Permission Denied

**Symptom**: Bedrock agent can't invoke Lambda

**Fix**:
1. Go to Lambda function â†’ Configuration â†’ Permissions
2. Check "Resource-based policy statements"
3. Ensure policy exists with Principal: `bedrock.amazonaws.com`
4. If missing, add:
   - Statement ID: `bedrock-invoke`
   - Principal: `bedrock.amazonaws.com`
   - Action: `lambda:InvokeFunction`

### Issue 2: Agent Can't Find S3 Schemas

**Symptom**: Agent creation fails on schema validation

**Fix**:
1. Verify schemas exist: `aws s3 ls s3://strands-etl-schemas/schemas/`
2. Check Bedrock Agent Role has S3 read permissions
3. Ensure schema S3 URI is correct: `s3://strands-etl-schemas/schemas/[filename].json`

### Issue 3: No Learning Vectors Found

**Symptom**: Decision agent reports no similar workloads

**Fix**:
1. Verify vectors exist: `aws s3 ls s3://strands-etl-learning/learning/vectors/learning/`
2. Upload sample vectors from Step 22
3. Check Lambda has S3 read permissions on learning bucket

### Issue 4: Dashboard Won't Connect

**Symptom**: Dashboard shows AWS credential errors

**Fix**:
1. Run: `aws configure list`
2. Verify region is `us-east-1`
3. Test credentials: `aws sts get-caller-identity`
4. Check IAM user has Bedrock:InvokeAgent permission

### Issue 5: Bedrock Model Not Available

**Symptom**: Agent creation fails - model not found

**Fix**:
1. Go to Bedrock â†’ Model access
2. Verify "Claude 3.5 Sonnet v2" shows "Access granted"
3. If pending, wait 1-5 minutes and refresh
4. If denied, check your AWS account limits

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER REQUEST                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Supervisor Agent    â”‚
              â”‚   (Orchestrator)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚              â”‚
         â–¼               â–¼               â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Decision â”‚    â”‚ Quality â”‚    â”‚Optimiza-â”‚   â”‚Learning â”‚
    â”‚ Agent   â”‚    â”‚ Agent   â”‚    â”‚tion     â”‚   â”‚ Agent   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚             â”‚
         â–¼              â–¼              â–¼             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Decision â”‚    â”‚ Quality â”‚    â”‚Optimiza-â”‚   â”‚Learning â”‚
    â”‚ Lambda  â”‚    â”‚ Lambda  â”‚    â”‚tion     â”‚   â”‚ Lambda  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€Lambdaâ”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚             â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   S3 Learning        â”‚
              â”‚   Vectors Storage    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Files Reference

**Configuration**:
- Lambda code: `/home/user/strands_etl/lambda_functions/decision/handler.py`
- Agent configs: `/home/user/strands_etl/bedrock_agents/configs/`
- API schemas: `/home/user/strands_etl/bedrock_agents/schemas/`
- Dashboard: `/home/user/strands_etl/dashboard/strands_dashboard.py`

**AWS Resources**:
- S3 Buckets: `strands-etl-learning`, `strands-etl-schemas`
- IAM Roles: `StrandsLambdaExecutionRole`, `StrandsBedrockAgentRole`
- Lambda Functions: All prefixed with `strands-`
- Bedrock Agents: All prefixed with `strands-`

---

## Support & Documentation

**Full Guides**:
- Architecture details: `BEDROCK_AGENTS_README.md`
- Cost breakdown: `AWS_PROVISIONING_GUIDE.md`
- Implementation plan: `BEDROCK_IMPLEMENTATION_PLAN.md`

**GitHub Repository**:
- Branch: `claude/bedrock-s3-only-mklys5sfa8e4xuqc-u0a48`
- PR: https://github.com/dhilipansankaralingam-del/strands_etl/pull/new/claude/bedrock-s3-only-mklys5sfa8e4xuqc-u0a48

---

## Success Metrics

After completion, you should have:

âœ… All 5 agents responding to test queries
âœ… Supervisor agent coordinating multi-agent workflows
âœ… Learning vectors stored in S3
âœ… Dashboard displaying real-time metrics
âœ… Complete ETL pipeline orchestration working
âœ… Monthly cost under $1,240

---

**Document Version**: 1.0
**Last Updated**: 2024-01-21
**Total Setup Time**: 3-4 hours
**Difficulty**: Beginner to Intermediate
**Monthly Cost**: $570-1,240
